<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style>
  html {
    scroll-behavior: smooth;
  }
  </style>


  <title>StyleDrive</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">



  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
<nav class="navbar is-fixed-top custom-navbar" role="navigation" aria-label="main navigation">
  <div class="container is-fluid px-5">
    <div class="navbar-brand">
      <a class="navbar-item site-name" href="#">
        StyleDrive
      </a>

      <!-- Mobile menu toggle -->
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarMenu">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div id="navbarMenu" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item nav-link" href="#Abstract">Abstract</a>
        <a class="navbar-item nav-link" href="#Method">Method</a>
        <a class="navbar-item nav-link" href="#Dataset">Dataset</a>
        <a class="navbar-item nav-link" href="#Videos">Videos</a>
        <a class="navbar-item nav-link" href="#Benchmark">Benchmark</a>
        <a class="navbar-item nav-link" href="#CaseStudy">Case Study</a>
        <a class="navbar-item nav-link" href="#BibTeX">BibTeX</a>
        <a class="navbar-item nav-link" href="https://huggingface.co/datasets/Ryhn98/StyleDrive-Dataset" target="_blank">Hugging Face</a>
        <a class="navbar-item nav-link" href="https://github.com/AIR-THU/StyleDrive" target="_blank">GitHub</a>
      </div>
    </div>
  </div>
  </div>
</nav>




  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">StyleDrive: Towards Driving-Style Aware
Benchmarking of End-To-End Autonomous Driving</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://ry-hao.top/" target="_blank" style="color: #6a0dad; font-weight: 500;">Ruiyang Hao</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://arthur12137.com/" target="_blank" style="color: #6a0dad; font-weight: 500;">Bowen Jing</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=JW4F5HoAAAAJ" target="_blank" style="color: #6a0dad; font-weight: 500;">Haibao Yu</a><sup>1,3</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=Qg7T6vUAAAAJ" target="_blank" style="color: #6a0dad; font-weight: 500;">Zaiqing Nie</a><sup>1,*</sup></span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>AIR, Tsinghua University, <sup>2</sup>The University of Manchester, <sup>3</sup>The University of Hong Kong<br></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup></small>Corresponding to zaiqing@air.tsinghua.edu.cn.</span>
                  </div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2506.23982" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                        </a>
                      </span>

                    <!-- Supplementary PDF link -->
                      <!-- Dataset link with custom dataset icon -->
                      <span class="link-block">
                        <a href="https://huggingface.co/datasets/Ryhn98/StyleDrive-Dataset" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <img src="https://cdn-icons-png.flaticon.com/512/16630/16630912.png" alt="Dataset Icon" style="height: 1em;">
                          </span>
                          <span>Dataset</span>
                        </a>
                      </span>


                      <span class="link-block">
                        <a href="https://huggingface.co/datasets/Ryhn98/StyleDrive-Dataset" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="height: 1em;">
                          </span>
                          <span>Hugging Face</span>
                        </a>
                      </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/AIR-THU/StyleDrive" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->

              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light" id="Abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 mb-5">Abstract</h2>
        <div class="content has-text-justified is-size-5" style="line-height: 1.8;">
          
          <p>
            Despite the growing capability of <strong>end-to-end autonomous driving (E2EAD)</strong> systems, <strong>personalization remains largely overlooked</strong>.
            Aligning driving behavior with individual user preferences is essential for comfort, trust, and real-world deployment—
            but existing datasets and benchmarks lack the necessary structure and scale to support this goal.
          </p>

          <p>
            <strong>StyleDrive</strong> bridges this gap with the following key contributions:
          </p>

          <ul style="list-style: none; padding-left: 0;">
            <li class="mb-3">
              <span style="font-weight: bold; color: #6a0dad;">Large-scale dataset:</span>
              A real-world dataset for personalized E2EAD, annotated with both objective behavior and subjective style preferences across diverse traffic scenarios.
            </li>
            <li class="mb-3">
              <span style="font-weight: bold; color: #6a0dad;">Multi-stage annotation:</span>
              A hybrid pipeline combining rule-based heuristics, vision-language model reasoning, and human verification to ensure consistent and interpretable labels.
            </li>
            <li class="mb-3">
              <span style="font-weight: bold; color: #6a0dad;">Personalized benchmark:</span>
              The first benchmark for style-conditioned driving evaluation across different model families.
            </li>
            <li class="mb-3">
              <span style="font-weight: bold; color: #6a0dad;">Empirical validation:</span>
              Extensive experiments show style-aware models align better with human behavior, proving the value of personalization in autonomy.
            </li>
          </ul>
          
        </div>
      </div>
    </div>
  </div>
</section>



<!-- End paper abstract -->

<!-- Paper Overview Image -->
<section class="section hero is-light" id="Method">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Method Overview</h2>
        <figure class="image">
          <img src="static/images/overview.png" alt="Overview Figure" style="width: 100%; height: auto;">
        </figure>
        <p class="has-text-justified is-size-6 mt-4">
          The figure illustrates the motivation and overview of StyleDrive.
          Users increasingly expect AVs not just to drive safely—but to drive like them. 
          Integrating personalization into E2EAD is challenging due to 
          (1) the lack of real-world datasets with style annotations enabling E2EAD, and 
          (2) limited architectures consider style preference as condition in E2E manner.  
          To bridge this gap, we present the first real-world dataset and benchmark tailored for personalized E2EAD.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section" id="Dataset">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3 mb-5">The StyleDrive Dataset</h2>

        <div class="content has-text-justified is-size-5" style="line-height: 1.8;">
          <p>
            <strong>StyleDrive</strong> is the first large-scale real-world dataset tailored for <strong>personalized end-to-end autonomous driving (E2EAD)</strong>.
            It is constructed on top of the <strong>OpenScene</strong> dataset, and contains nearly <strong>30,000 driving scenarios</strong> across urban and rural environments.
          </p>

          <p>
            Each scenario is annotated with both:
          </p>

          <ul style="list-style: none; padding-left: 0;">
            <li class="mb-2">
              <span style="font-weight: bold; color: #6a0dad;">Objective behavioral features:</span>
              e.g. velocity, acceleration, yaw rate, lane position, motion history.
            </li>
            <li class="mb-2">
              <span style="font-weight: bold; color: #6a0dad;">Subjective driving style preferences:</span>
              categorized as <em>Aggressive</em>, <em>Normal</em>, or <em>Conservative</em>.
            </li>
          </ul>

          <p>
            To ensure high-quality and interpretable annotations, we propose a <strong>multi-stage labeling pipeline</strong>:
          </p>

          <ul style="list-style: none; padding-left: 0;">
            <li class="mb-2">
              <span style="font-weight: bold; color: #6a0dad;">Rule-based heuristics:</span>
              Classify motion patterns using interpretable thresholds (speed, jerk, safety ratio) under each traffic scenario.
            </li>
            <li class="mb-2">
              <span style="font-weight: bold; color: #6a0dad;">VLM-based reasoning:</span>
              Leverages a fine-tuned vision-language model to extract <em>semantic intents</em> from bird's-eye view images.
            </li>
            <li class="mb-2">
              <span style="font-weight: bold; color: #6a0dad;">Human-in-the-loop verification:</span>
              Final labels are verified and calibrated via human annotators for robustness and consistency.
            </li>
          </ul>

          <p>
            The dataset spans <strong>11 real-world traffic scenario types</strong> (e.g., intersections, merges, roundabouts), and provides per-scenario safety scores, relative positioning, and temporal style labels.
          </p>

          <p>
            With its hybrid annotation strategy and diverse behavior coverage, <strong>StyleDrive</strong> offers a unique resource for modeling, analyzing, and benchmarking <strong>human-aligned autonomous driving</strong>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered mb-5">Dataset Construction</h2>

        <div class="content has-text-justified is-size-5" style="line-height: 1.8;">
          <p>
            The <strong>StyleDrive</strong> dataset is constructed via a <strong>multi-stage annotation pipeline</strong> designed to capture both low-level behavior and high-level style preferences, grounded in rich semantic contexts.
          </p>

          <p>
            The process begins by segmenting raw driving clips into <strong>traffic scenarios</strong> using high-definition map topology. Within each segment, <strong>semantic context</strong>—such as proximity to lead vehicles, pedestrians, or lane merges—is extracted through a fine-tuned <strong>Vision-Language Model (VLM)</strong>, enabling interpretable behavioral grounding.
          </p>

          <p><strong>Style labeling is performed in three key stages:</strong></p>

          <ul style="list-style: none; padding-left: 0;">
            <li class="mb-3">
              <span style="font-weight: bold; color: #6a0dad;">Rule-based analysis:</span>
              Apply scenario-specific thresholds over motion features (speed, acceleration, yaw rate) to detect patterns like rapid acceleration or risky merges.
            </li>

            <li class="mb-3">
              <span style="font-weight: bold; color: #6a0dad;">VLM-based semantic inference:</span>
              Use VLMs to reason over BEV semantic images and infer behavioral intent via multimodal prompts.
            </li>

            <li class="mb-3">
              <span style="font-weight: bold; color: #6a0dad;">Human-in-the-loop refinement:</span>
              Annotation outputs are reviewed and refined by human annotators to ensure style consistency and realism.
            </li>
          </ul>

          <p>
            A <strong>risk-aware fusion strategy</strong> combines rule-based and VLM outputs, yielding consistent, interpretable, and scalable style labels. The final dataset includes style annotations across <strong>11 real-world traffic scenario types</strong>, enabling robust personalized driving policy learning.
          </p>
        </div>

        <!-- Annotation Framework Image -->
        <figure class="image mt-5">
          <img src="static/images/anno_framework.png" alt="Annotation Framework Diagram" style="width: 100%; height: auto; border-radius: 12px; box-shadow: 0 8px 24px rgba(0,0,0,0.08);">
        </figure>
        <p class="has-text-centered is-size-6 mt-2" style="color: #666;">
          <em>Annotation pipeline integrating topology segmentation, semantic extraction, rule-based analysis, VLM inference, and human verification.</em>
        </p>

      </div>
    </div>
  </div>
</section>


<section class="section" >
  <div class="container" style="max-width: 860px;">
    <h2 class="title is-3 has-text-centered mb-5">Dataset Structure</h2>

    <div class="content has-text-justified is-size-5" style="line-height: 1.8;">
      <p>
        Each annotated sample in the <strong>StyleDrive</strong> dataset encapsulates a wide array of <strong>semantic, dynamic, and contextual attributes</strong>, enabling rich representation of driving behaviors and personalized preferences. The data schema is designed to support learning models that condition on <em>style, motion, perception, and safety cues</em>.
      </p>
      <p>
        Below is a representative structure of one data entry:
      </p>
    </div>

    <!-- 表格展示 -->
    <div class="table-container mt-4" style="overflow-x: auto;">
      <table class="table is-bordered is-striped is-fullwidth is-size-7" style="font-family: monospace;">
        <thead>
          <tr>
            <th style="width: 32%;">Field</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>vx_ego, vy_ego, v_ego</td><td>Velocity vector (x, y) and magnitude of ego vehicle</td></tr>
          <tr><td>ax_ego, ay_ego, a_ego</td><td>Acceleration vector and overall acceleration</td></tr>
          <tr><td>yaw, yaw_diff</td><td>Heading angle and angular change across frames</td></tr>
          <tr><td>v_avg, v_std</td><td>Average and variability of velocity (last 10 frames)</td></tr>
          <tr><td>vy_max</td><td>Peak lateral velocity (side-slip indicator)</td></tr>
          <tr><td>a_max, a_std, ax_avg</td><td>Max, std of acceleration and average forward acceleration</td></tr>
          <tr><td>ini_direction_judge</td><td>Initial motion direction classification</td></tr>
          <tr><td>scenario_type</td><td>Semantic scenario type parsed from VLM (e.g., roundabout, crosswalk)</td></tr>
          <tr><td>scene_token</td><td>Globally unique identifier for each driving scene</td></tr>
          <tr><td>has_left_rear / right_rear</td><td>Flags indicating rear adjacent vehicles</td></tr>
          <tr><td>left_rear_min / right_rear_min</td><td>Minimum distances to left/right rear vehicles</td></tr>
          <tr><td>speed_mode</td><td>Preferred driving style label (Aggressive, Normal, Conservative)</td></tr>
          <tr><td>front_frame</td><td>Temporal distance series to front vehicle</td></tr>
          <tr><td>safe_frame</td><td>Boolean safety labels per frame</td></tr>
          <tr><td>safe_ratio, unsafe_ratio, oversafe_ratio</td><td>Style-based safety distribution over entire scenario</td></tr>
        </tbody>
      </table>
    </div>

    <p class="is-size-6 mt-4 has-text-centered" style="color: #666;">
      <em>The structured schema supports learning from motion, interaction, semantics, and style.</em>
    </p>
  </div>
</section>


<section class="section" id="Videos">
  <div class="container is-max-desktop">
    <h3 class="title is-3 has-text-centered mb-3">Traffic Scenario Videos</h3>
    <p class="is-size-5 has-text-centered mb-5 has-text-grey">
      <strong>StyleDrive</strong> includes <strong>11 diverse real-world traffic scenarios</strong>, capturing personalized driving behaviors across structured and unstructured environments.
    </p>

    <!-- 视频轮播区域 -->
    <div id="video-carousel" class="carousel results-carousel">

      <!-- Crosswalks -->
      <div class="item has-text-centered px-2">
        <div class="box" style="border-radius: 16px; box-shadow: 0 8px 24px rgba(0,0,0,0.05); padding: 1rem;">
          <video autoplay muted loop playsinline preload="none" loading="lazy" width="100%" style="border-radius: 12px;">
            <source src="static/videos/067328_1629228525900693_5848b2ff8a1959f0.mp4" type="video/mp4">
          </video>
          <p class="mt-3"><strong>Crosswalks</strong><br><span class="is-size-7 has-text-grey">Navigating pedestrian crossings with varying yielding behaviors</span></p>
        </div>
      </div>

      <!-- Lane Following -->
      <div class="item has-text-centered px-2">
        <div class="box" style="border-radius: 16px; box-shadow: 0 8px 24px rgba(0,0,0,0.05); padding: 1rem;">
          <video autoplay muted loop playsinline preload="none" loading="lazy" width="100%" style="border-radius: 12px;">
            <source src="static/videos/075248_1624471344901481_fc67412be3615e37.mp4" type="video/mp4">
          </video>
          <p class="mt-3"><strong>Lane Following</strong><br><span class="is-size-7 has-text-grey">Maintaining lane center under speed and curvature variations</span></p>
        </div>
      </div>

      <!-- Protected Intersections -->
      <div class="item has-text-centered px-2">
        <div class="box" style="border-radius: 16px; box-shadow: 0 8px 24px rgba(0,0,0,0.05); padding: 1rem;">
          <video autoplay muted loop playsinline preload="none" loading="lazy" width="100%" style="border-radius: 12px;">
            <source src="static/videos/077952_1624460617600689_1ffb98a4f73b58bc.mp4" type="video/mp4">
          </video>
          <p class="mt-3"><strong>Protected Intersections</strong><br><span class="is-size-7 has-text-grey">Turning or crossing with clear traffic signals or stop signs</span></p>
        </div>
      </div>

      <!-- Unprotected Intersections -->
      <div class="item has-text-centered px-2">
        <div class="box" style="border-radius: 16px; box-shadow: 0 8px 24px rgba(0,0,0,0.05); padding: 1rem;">
          <video autoplay muted loop playsinline preload="none" loading="lazy" width="100%" style="border-radius: 12px;">
            <source src="static/videos/064520_1630418176799823_9dffe4e7a06a5c6e.mp4" type="video/mp4">
          </video>
          <p class="mt-3"><strong>Unprotected Intersections</strong><br><span class="is-size-7 has-text-grey">Negotiating multi-agent crossings without explicit right-of-way</span></p>
        </div>
      </div>

      <!-- Lane Change -->
      <div class="item has-text-centered px-2">
        <div class="box" style="border-radius: 16px; box-shadow: 0 8px 24px rgba(0,0,0,0.05); padding: 1rem;">
          <video autoplay muted loop playsinline preload="none" loading="lazy" width="100%" style="border-radius: 12px;">
            <source src="static/videos/079168_1626454279599838_0afd7a88e5d75e86.mp4" type="video/mp4">
          </video>
          <p class="mt-3"><strong>Lane Change</strong><br><span class="is-size-7 has-text-grey">Executing safe or assertive lane shifts in dense or sparse traffic</span></p>
        </div>
      </div>

      <!-- Side Ego to Main -->
      <div class="item has-text-centered px-2">
        <div class="box" style="border-radius: 16px; box-shadow: 0 8px 24px rgba(0,0,0,0.05); padding: 1rem;">
          <video autoplay muted loop playsinline preload="none" loading="lazy" width="100%" style="border-radius: 12px;">
            <source src="static/videos/028488_1626398883599632_7f5c568556895ccf.mp4" type="video/mp4">
          </video>
          <p class="mt-3"><strong>Side Ego to Main</strong><br><span class="is-size-7 has-text-grey">Entering a main road from a side street with merging considerations</span></p>
        </div>
      </div>

      <!-- Side to Main Ego -->
      <div class="item has-text-centered px-2">
        <div class="box" style="border-radius: 16px; box-shadow: 0 8px 24px rgba(0,0,0,0.05); padding: 1rem;">
          <video autoplay muted loop playsinline preload="none" loading="lazy" width="100%" style="border-radius: 12px;">
            <source src="static/videos/072712_1623279250300850_d894416d0aa559ad.mp4" type="video/mp4">
          </video>
          <p class="mt-3"><strong>Side to Main Ego</strong><br><span class="is-size-7 has-text-grey">Ego vehicle on main road encounters merging side traffic</span></p>
        </div>
      </div>

      <!-- Special Interior Road -->
      <div class="item has-text-centered px-2">
        <div class="box" style="border-radius: 16px; box-shadow: 0 8px 24px rgba(0,0,0,0.05); padding: 1rem;">
          <video autoplay muted loop playsinline preload="none" loading="lazy" width="100%" style="border-radius: 12px;">
            <source src="static/videos/051476_1626457042600513_0ee4ce6ccacd5074.mp4" type="video/mp4">
          </video>
          <p class="mt-3"><strong>Special Interior Road</strong><br><span class="is-size-7 has-text-grey">Driving through complex local environments like alleys, parking, or loops</span></p>
        </div>
      </div>

      <!-- Countryside Road -->
      <div class="item has-text-centered px-2">
        <div class="box" style="border-radius: 16px; box-shadow: 0 8px 24px rgba(0,0,0,0.05); padding: 1rem;">
          <video autoplay muted loop playsinline preload="none" loading="lazy" width="100%" style="border-radius: 12px;">
            <source src="static/videos/078708_1629219116799692_4bfe81933d245ba6.mp4" type="video/mp4">
          </video>
          <p class="mt-3"><strong>Countryside Road</strong><br><span class="is-size-7 has-text-grey">Navigating long, curved roads with sparse surrounding vehicles</span></p>
        </div>
      </div>

      <!-- Roundabout Interior -->
      <div class="item has-text-centered px-2">
        <div class="box" style="border-radius: 16px; box-shadow: 0 8px 24px rgba(0,0,0,0.05); padding: 1rem;">
          <video autoplay muted loop playsinline preload="none" loading="lazy" width="100%" style="border-radius: 12px;">
            <source src="static/videos/024812_1633119850699512_387ac2febd8e51a6.mp4" type="video/mp4">
          </video>
          <p class="mt-3"><strong>Roundabout Interior</strong><br><span class="is-size-7 has-text-grey">Maintaining trajectory inside multi-lane roundabout with potential exits</span></p>
        </div>
      </div>

      <!-- Roundabout Entrance -->
      <div class="item has-text-centered px-2">
        <div class="box" style="border-radius: 16px; box-shadow: 0 8px 24px rgba(0,0,0,0.05); padding: 1rem;">
          <video autoplay muted loop playsinline preload="none" loading="lazy" width="100%" style="border-radius: 12px;">
            <source src="static/videos/037048_1633103723299995_b6556bf2248c5e02.mp4" type="video/mp4">
          </video>
          <p class="mt-3"><strong>Roundabout Entrance</strong><br><span class="is-size-7 has-text-grey">Merging into circular flow under dynamic vehicle interactions</span></p>
        </div>
      </div>

    </div>
  </div>
</section>




<section class="section" id="Benchmark">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">StyleDrive Benchmark</h2>
        <div class="content has-text-justified is-size-6" style="line-height: 1.8;">
          <p>
            The <strong style="color: #6a0dad;">StyleDrive Benchmark</strong> introduces a standardized evaluation suite for <strong>personalized end-to-end autonomous driving</strong>. It extends the StyleDrive dataset into a closed-loop testing environment built upon the <strong style="color: #6a0dad;">NavSim simulator</strong>, allowing models to be evaluated in realistic, interactive traffic scenes.
          </p>

          <p>
            At the core of this benchmark is the <strong style="color: #6a0dad;">Style-Modulated Predictive Driver Model Score (SM-PDMS)</strong>. This metric jointly captures:
          </p>

          <ul style="margin-left: 1rem;">
            <li><strong>Feasibility:</strong> including lane adherence, goal completion, and safety (e.g., collision-free trajectory, time-to-collision).</li>
            <li><strong>Style Alignment:</strong> assessing how well the generated behaviors reflect target preferences across axes such as comfort, caution, and responsiveness.</li>
          </ul>

          <p>
            Evaluation is conducted under three canonical driving styles — <strong style="color: #6a0dad;">Aggressive</strong>, <strong style="color: #6a0dad;">Normal</strong>, and <strong style="color: #6a0dad;">Conservative</strong>. Each policy is conditioned on a target style label and deployed in multiple real-world driving contexts.
          </p>

          <p>
            We implement baseline controllers spanning multiple model families — including MLP-based predictors, transformer architectures, and diffusion-policy networks. Experimental results consistently demonstrate that <strong style="color: #6a0dad;">style-aware models</strong> achieve higher SM-PDMS scores and produce behaviors more closely aligned with human demonstrations.
          </p>

          <p>
            This benchmark provides a reproducible, quantitative platform for evaluating <strong>style-conditioned policy learning</strong>. It serves as a crucial step toward developing <strong>human-aligned, preference-aware driving agents</strong> in real-world environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container" style="max-width: 860px;">

    <!-- Main Results Table -->
    <div class="content has-text-centered">
      <h3 class="title is-4">Benchmark Performance across Model Families</h3>
      <p class="is-size-6">
        The following table summarizes the performance of various E2EAD models under <strong style="color:#6a0dad">style-conditioned evaluation</strong> using the SM-PDMS framework. Higher scores indicate better feasibility and alignment with target driving preferences.
      </p>
    </div>

    <div class="table-container" style="overflow-x: auto;">
      <table class="table is-bordered is-striped is-fullwidth is-size-7 has-text-centered">
        <thead>
          <tr>
            <th>Model</th>
            <th>NC ↑</th>
            <th>DAC ↑</th>
            <th>TTC ↑</th>
            <th>Comf. ↑</th>
            <th>EP ↑</th>
            <th><span style="color:#6a0dad">SM-PDMS ↑</span></th>
          </tr>
        </thead>
        <tbody>
          <tr><td>AD-MLP</td><td>92.63</td><td>77.68</td><td>83.83</td><td>99.75</td><td>78.01</td><td>63.72</td></tr>
          <tr><td>TransFuser</td><td>96.74</td><td>88.43</td><td>91.08</td><td>99.65</td><td>84.39</td><td>78.12</td></tr>
          <tr><td>WoTE</td><td>97.29</td><td>92.39</td><td>92.53</td><td>99.13</td><td>76.31</td><td>79.56</td></tr>
          <tr><td>DiffusionDrive</td><td>96.66</td><td>91.45</td><td>90.63</td><td>99.73</td><td>80.39</td><td>79.33</td></tr>
          <tr><td>AD-MLP-Style</td><td>92.38</td><td>73.23</td><td>83.14</td><td><span style="color: red">99.90</span></td><td>78.55</td><td>60.02</td></tr>
          <tr><td>TransFuser-Style</td><td>97.23</td><td>90.36</td><td>92.61</td><td>99.73</td><td><span style="color: red">84.95</span></td><td>81.09</td></tr>
          <tr><td>WoTE-Style</td><td><span style="color: blue">97.58</span></td><td><span style="color: blue">93.44</span></td><td><span style="color: blue">93.70</span></td><td>99.26</td><td>77.38</td><td><span style="color: blue">81.38</span></td></tr>
          <tr><td>DiffusionDrive-Style</td><td><span style="color: red">97.81</span></td><td><span style="color: red">93.45</span></td><td><span style="color: red">92.81</span></td><td><span style="color: red">99.85</span></td><td><span style="color: red">84.84</span></td><td><span style="color: red">84.10</span></td></tr>
        </tbody>
      </table>
    </div>

    <!-- Human Alignment Table -->
    <div class="content has-text-centered mt-6">
      <h3 class="title is-4">Trajectory Consistency with Human Demonstrations</h3>
      <p class="is-size-6">
        We further evaluate <strong style="color:#6a0dad">trajectory-level similarity</strong> via L2 error across prediction horizons. Style-aware models consistently reduce average L2 distance, indicating stronger alignment with human-like driving behavior.
      </p>
    </div>

    <div class="table-container" style="overflow-x: auto;">
      <table class="table is-bordered is-striped is-fullwidth is-size-7 has-text-centered">
        <thead>
          <tr>
            <th>Model</th>
            <th>L2 (2s) ↓</th>
            <th>L2 (3s) ↓</th>
            <th>L2 (4s) ↓</th>
            <th>L2 Avg ↓</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>WoTE</td><td>0.733</td><td>1.434</td><td>2.349</td><td>1.506</td></tr>
          <tr><td>AD-MLP</td><td>0.503</td><td>1.262</td><td>2.383</td><td>1.382</td></tr>
          <tr><td>TransFuser</td><td>0.431</td><td>0.963</td><td>1.701</td><td>1.032</td></tr>
          <tr><td>DiffusionDrive</td><td>0.471</td><td>1.086</td><td>1.945</td><td>1.167</td></tr>
          <tr><td>WoTE-Style</td><td>0.673</td><td>1.340</td><td>2.223</td><td>1.412</td></tr>
          <tr><td>AD-MLP-Style</td><td>0.510</td><td>1.230</td><td>2.321</td><td>1.354</td></tr>
          <tr><td>TransFuser-Style</td><td>0.424</td><td>0.937</td><td>1.656</td><td>1.006</td></tr>
          <tr><td>DiffusionDrive-Style</td>
              <td style="font-weight: bold;">0.417</td>
              <td style="font-weight: bold;">0.940</td>
              <td style="font-weight: bold;">1.646</td>
              <td style="font-weight: bold;">1.001</td>
          </tr>
        </tbody>
      </table>
    </div>

    <br><br>
  </div>
</section>



<section class="section" id="CaseStudy">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Qualitative Case Study of Style Effects</h2>

        <p class="is-size-6 has-text-justified mb-5">
          To further analyze the impact of style-conditioning, we visualize <strong style="color:#6a0dad">DiffusionDrive-Style</strong> predictions under controlled scenario conditions. The same traffic environment is used while varying only the driving style input: <strong style="color:#6a0dad">Aggressive</strong>, <strong style="color:#6a0dad">Normal</strong>, and <strong style="color:#6a0dad">Conservative</strong>.
        </p>

        <figure class="image mb-4">
          <img src="static/images/Policy_Cases.png" alt="Style-Conditioned Trajectories" style="width: 100%; border-radius: 12px; box-shadow: 0 6px 24px rgba(0,0,0,0.06);">
        </figure>

        <p class="is-size-6 has-text-justified">
          <strong style="color:#fa0000">Red dotted lines</strong> represent predicted trajectories generated by the model under each style condition, while <strong class="has-text-success">green dotted lines</strong> show the corresponding human demonstrations. Distinct patterns emerge:
          <ul class="mt-3" style="margin-left: 1rem;">
            <li><strong>Aggressive:</strong> higher speeds, sharper turns, assertive merging behavior.</li>
            <li><strong>Normal:</strong> stable lane following, moderate acceleration, balanced risk.</li>
            <li><strong>Conservative:</strong> longer following distances, reduced velocity, safer decisions at intersections.</li>
          </ul>
          These visual comparisons illustrate the <strong style="color:#6a0dad">model's controllability</strong> and its capability to align outputs with high-level human preferences across complex driving scenes.
        </p>

      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Dataset View</h2>
        <figure class="image mt-4">
          <img src="static/images/alter.png" alt="Alternative Annotation Framework" style="width: 100%; height: auto;">
        </figure>
        <p class="has-text-justified is-size-6 mt-3">
          <!-- <em>An alternative visualization of the annotation process, highlighting semantic extraction, behavioral labeling, and human-in-the-loop fusion.</em> -->
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->

<!-- End image carousel -->












<!--BibTex citation -->
  <section class="section" id="BibTeX" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code> @article{hao2025styledrive,
  title={StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving},
  author={Hao, Ruiyang and Jing, Bowen and Yu, Haibao and Nie, Zaiqing},
  journal={arXiv preprint arXiv:2506.23982},
  year={2025}
}</code></pre>
    </div>
</section>

<footer class="footer has-text-centered footer-affiliation">
  <h2 class="title is-5 mb-4">Affiliations</h2>

  <!-- 校徽图标部分 -->
  <div class="columns is-mobile is-centered is-vcentered mb-3">
    <div class="column is-narrow mx-3">
      <img src="static/images/logo_thu.png" alt="THU" class="footer-logo">
    </div>
    <div class="column is-narrow mx-3">
      <img src="static/images/logo_uom.png" alt="UoM" class="footer-logo">
    </div>
    <div class="column is-narrow mx-3">
      <img src="static/images/logo_hku.png" alt="HKU" class="footer-logo">
    </div>
  </div>

  <!-- 单位文字 -->
  <p class="is-size-6 has-text-grey">
    Tsinghua University · University of Manchester · University of Hong Kong
  </p>
</footer>




<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
<script>
  bulmaCarousel.attach('#video-carousel', {
    slidesToScroll: 1,
    slidesToShow: 1,
    autoplay: false,  // 你可以改成 true
    loop: true,
    pauseOnHover: true,
    navigation: true
  });
</script>

<script>
document.addEventListener('DOMContentLoaded', () => {
  const burger = document.querySelector('.navbar-burger');
  const menu = document.getElementById('navbarMenu');

  if (burger && menu) {
    burger.addEventListener('click', () => {
      burger.classList.toggle('is-active');
      menu.classList.toggle('is-active');
    });
  }
});
</script>



  </body>
</html>
