<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>StyleDrive</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
<nav class="navbar is-fixed-top has-background-purple" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a class="navbar-item has-text-white" href="#">
      <strong>StyleDrive</strong>
    </a>
  </div>

  <div class="navbar-menu">
    <div class="navbar-start">
      <a class="navbar-item has-text-white" href="#BibTeX">
        BibTeX
      </a>
      <a class="navbar-item has-text-white" href="#video-carousel">
        Videos
      </a>
      <a class="navbar-item has-text-white" href="#Dataset">
        Dataset
      </a>
    </div>
  </div>
</nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">StyleDrive: Towards Driving-Style Aware
Benchmarking of End-To-End Autonomous Driving</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://ry-hao.top/" target="_blank">Ruiyang Hao</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://arthur12137.com/" target="_blank">Bowen Jing</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=JW4F5HoAAAAJ" target="_blank">Haibao Yu</a><sup>1,3</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=Qg7T6vUAAAAJ" target="_blank">Zaiqing Nie</a><sup>1,*</sup></span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>AIR, Tsinghua University, <sup>2</sup>The University of Manchester, <sup>3</sup>The University of Hong Kong<br></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup></small>Corresponding to zaiqing@air.tsinghua.edu.cn.</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2506.23982" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                        </a>
                      </span>

                    <!-- Supplementary PDF link -->
                      <!-- Dataset link with custom dataset icon -->
                      <span class="link-block">
                        <a href="https://huggingface.co/datasets/Ryhn98/StyleDrive-Dataset" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <img src="https://cdn-icons-png.flaticon.com/512/16630/16630912.png" alt="Dataset Icon" style="height: 1em;">
                          </span>
                          <span>Dataset</span>
                        </a>
                      </span>


                      <span class="link-block">
                        <a href="https://huggingface.co/datasets/Ryhn98/StyleDrive-Dataset" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="height: 1em;">
                          </span>
                          <span>Hugging Face</span>
                        </a>
                      </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/AIR-THU/StyleDrive" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->

              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified is-size-6">
          <p>
            Despite the growing capability of end-to-end autonomous driving (E2EAD) systems, personalization remains largely overlooked. Aligning driving behavior with individual user preferences is essential for comfort, trust, and real-world deployment, but existing datasets and benchmarks lack the necessary structure and scale to support this goal.
          </p>
          <p>
            <strong>StyleDrive</strong> addresses this gap through the following key contributions:
          </p>
          <ul>
            <li><strong>A novel large-scale real-world dataset</strong> for personalized E2EAD, annotated with both objective behaviors and subjective driving style preferences across diverse traffic scenarios.</li>

            <li><strong>A multi-stage annotation pipeline</strong> combining rule-based analysis, visual language model (VLM) reasoning, and human-in-the-loop verification to ensure consistent and interpretable style labels.</li>

            <li><strong>The first benchmark for personalized E2EAD</strong>, enabling standardized and quantitative comparison of style-conditioned driving behavior across different model architectures.</li>

            <li><strong>Comprehensive empirical results</strong> showing that style-aware models better align with human behavior, demonstrating the value of personalization for improved autonomy.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- End paper abstract -->

<!-- Paper Overview Image -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Method Overview</h2>
        <figure class="image">
          <img src="static/images/overview.png" alt="Overview Figure" style="width: 100%; height: auto;">
        </figure>
        <p class="has-text-justified is-size-6 mt-4">
          The figure illustrates the motivation and overview of StyleDrive.
          Users increasingly expect AVs not just to drive safely—but to drive like them. 
          Integrating personalization into E2EAD is challenging due to 
          (1) the lack of real-world datasets with style annotations enabling E2EAD, and 
          (2) limited architectures consider style preference as condition in E2E manner.  
          To bridge this gap, we present the first real-world dataset and benchmark tailored for personalized E2EAD.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">The StyleDrive Dataset</h2>
        <p class="has-text-justified is-size-6">
          <strong>StyleDrive</strong> is the first large-scale real-world dataset specifically designed to support personalized end-to-end autonomous driving. Built upon the OpenScene dataset, StyleDrive consists of nearly <strong>30,000 driving scenarios</strong> annotated with both <strong>objective behavioral signals</strong> and <strong>subjective driving style preferences</strong>. It captures diverse road conditions, semantic interactions, and motion patterns across urban and rural settings. By combining static road topology with dynamic context—extracted via a fine-tuned vision-language model (VLM)—StyleDrive enables detailed modeling of driving scenarios and nuanced behavior understanding. The annotations are produced through a hybrid pipeline that fuses rule-based heuristics with VLM-based reasoning, and are further verified through a human-in-the-loop process. This makes StyleDrive a <strong>high-quality, interpretable, and scalable resource</strong> for advancing research in human-centric autonomous driving.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Dataset Construction</h2>
        <div class="content has-text-justified is-size-6">
          <p>
            The <strong>StyleDrive</strong> dataset is built through a structured multi-stage pipeline designed to capture both objective driving behavior and subjective driver intent. It begins by categorizing driving clips into traffic scenario types using high-definition map topology. Dynamic contextual cues—such as lead vehicles, pedestrians, and merging risk—are extracted using a fine-tuned vision-language model (VLM), enabling rich semantic scene descriptions.
          </p>
          <p>
            Driving style labels are generated from two complementary sources:
          </p>
          <ul>
            <li><strong>Rule-based heuristics</strong> that analyze motion features (e.g., speed, acceleration, yaw rate) within scenario-specific thresholds;</li>
            <li><strong>VLM-based reasoning</strong>, which interprets high-level behavioral intent through multimodal prompts.</li>
          </ul>
          <p>
            These two annotation streams are fused using a risk-aware strategy and refined through human-in-the-loop verification, ensuring consistency, interpretability, and robustness. The final dataset provides per-scenario style annotations across 11 real-world driving contexts, supporting large-scale learning of personalized autonomous driving behavior.
          </p>
        </div>

        <!-- Annotation Framework Image -->
        <figure class="image mt-5">
          <img src="static/images/anno_framework.png" alt="Annotation Framework Diagram" style="width: 100%; height: auto;">
        </figure>
        <p class="has-text-justified is-size-6 mt-3">
          <em>Illustration of the annotation pipeline integrating topology, scene semantics, rule-based reasoning, VLM inference, and human verification.</em>
        </p>
        
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="max-width: 860px;">
    <h3 class="title is-4">Dataset Structure</h3>
    <p class="is-size-6">
      Each annotated instance in the StyleDrive dataset contains rich semantic, dynamic, and contextual attributes. Below is a sample structure of the data format:
    </p>

    <div class="table-container" style="overflow-x: auto;">
      <table class="table is-bordered is-fullwidth is-size-7" style="font-family: monospace;">
        <thead>
          <tr>
            <th>Field</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>vx_ego, vy_ego, v_ego</td><td>Ego vehicle velocity components (x, y) and magnitude</td></tr>
          <tr><td>ax_ego, ay_ego, a_ego</td><td>Ego acceleration components and magnitude</td></tr>
          <tr><td>yaw, yaw_diff</td><td>Heading angle and its temporal difference</td></tr>
          <tr><td>v_avg, v_std</td><td>Average and standard deviation of ego speed (last 10 frames)</td></tr>
          <tr><td>vy_max</td><td>Maximum lateral velocity magnitude</td></tr>
          <tr><td>a_max, a_std, ax_avg</td><td>Maximum acceleration, std of acceleration, average longitudinal acceleration</td></tr>
          <tr><td>ini_direction_judge</td><td>Initial motion direction category</td></tr>
          <tr><td>scenario_type</td><td>Scenario type from VLM semantic parsing</td></tr>
          <tr><td>scene_token</td><td>Unique scene identifier</td></tr>
          <tr><td>has_left_rear / right_rear</td><td>Boolean flags indicating vehicle presence in rear zones</td></tr>
          <tr><td>left_rear_min / right_rear_min</td><td>Minimum relative distance to rear vehicles</td></tr>
          <tr><td>speed_mode</td><td>Speed-related preference classification (e.g., aggressive, conservative)</td></tr>
          <tr><td>front_frame</td><td>Distance to front vehicle across frames</td></tr>
          <tr><td>safe_frame</td><td>Frame-wise safety flags</td></tr>
          <tr><td>safe_ratio, unsafe_ratio, oversafe_ratio</td><td>Distribution of safety judgments</td></tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <h3 class="title is-4 has-text-centered">Traffic Scenario Videos</h3>
    <p class="is-size-6 has-text-centered mb-4">
      The StyleDrive dataset covers 11 real-world driving scenarios, each exhibiting distinct challenges and personalized behaviors.
    </p>

    <div id="video-carousel" class="carousel results-carousel">
      <div class="item has-text-centered">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 10px;">
          <source src="static/videos/067328_1629228525900693_5848b2ff8a1959f0.mp4" type="video/mp4">
        </video>
        <p class="mt-2"><strong>Crosswalks</strong><br><span class="is-size-7">Navigating pedestrian crossings with varying yielding behaviors</span></p>
      </div>

      <div class="item has-text-centered">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 10px;">
          <source src="static/videos/075248_1624471344901481_fc67412be3615e37.mp4" type="video/mp4">
        </video>
        <p class="mt-2"><strong>Lane Following</strong><br><span class="is-size-7">Maintaining lane center under speed and curvature variations</span></p>
      </div>
      <div class="item has-text-centered">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 10px;">
          <source src="static/videos/077952_1624460617600689_1ffb98a4f73b58bc.mp4" type="video/mp4">
        </video>
        <p class="mt-2"><strong>Protected Intersections</strong><br><span class="is-size-7">Turning or crossing with clear traffic signals or stop signs</span></p>
      </div>
      <div class="item has-text-centered">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 10px;">
          <source src="static/videos/064520_1630418176799823_9dffe4e7a06a5c6e.mp4" type="video/mp4">
        </video>
        <p class="mt-2"><strong>Unprotected Intersections</strong><br><span class="is-size-7">Negotiating multi-agent crossings without explicit right-of-way</span></p>
      </div>
      <div class="item has-text-centered">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 10px;">
          <source src="static/videos/079168_1626454279599838_0afd7a88e5d75e86.mp4" type="video/mp4">
        </video>
        <p class="mt-2"><strong>Lane Change</strong><br><span class="is-size-7">Executing safe or assertive lane shifts in dense or sparse traffic</span></p>
      </div>
      <div class="item has-text-centered">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 10px;">
          <source src="static/videos/028488_1626398883599632_7f5c568556895ccf.mp4" type="video/mp4">
        </video>
        <p class="mt-2"><strong>Side Ego to Main</strong><br><span class="is-size-7">Entering a main road from a side street with merging considerations</span></p>
      </div>
      <div class="item has-text-centered">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 10px;">
          <source src="static/videos/072712_1623279250300850_d894416d0aa559ad.mp4" type="video/mp4">
        </video>
        <p class="mt-2"><strong>Side to Main Ego</strong><br><span class="is-size-7">Ego vehicle on main road encounters merging side traffic</span></p>
      </div>
      <div class="item has-text-centered">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 10px;">
          <source src="static/videos/051476_1626457042600513_0ee4ce6ccacd5074.mp4" type="video/mp4">
        </video>
        <p class="mt-2"><strong>Special Interior Road</strong><br><span class="is-size-7">Driving through complex local environments like alleys, parking, or loops</span></p>
      </div>
      <div class="item has-text-centered">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 10px;">
          <source src="static/videos/078708_1629219116799692_4bfe81933d245ba6.mp4" type="video/mp4">
        </video>
        <p class="mt-2"><strong>Countryside Road</strong><br><span class="is-size-7">Navigating long, curved roads with sparse surrounding vehicles</span></p>
      </div>
      <div class="item has-text-centered">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 10px;">
          <source src="static/videos/024812_1633119850699512_387ac2febd8e51a6.mp4" type="video/mp4">
        </video>
        <p class="mt-2"><strong>roundabout interior</strong><br><span class="is-size-7">Maintaining trajectory inside multi-lane roundabout with potential exits</span></p>
      </div>
      <div class="item has-text-centered">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 10px;">
          <source src="static/videos/037048_1633103723299995_b6556bf2248c5e02.mp4" type="video/mp4">
        </video>
        <p class="mt-2"><strong>roundabout Entrance</strong><br><span class="is-size-7">Merging into circular flow under dynamic vehicle interactions</span></p>
      </div>

      <!-- Repeat for other scenarios -->
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">StyleDrive Benchmark</h2>
        <div class="content has-text-justified is-size-6">
          <p>
            The <strong>StyleDrive Benchmark</strong> is a standardized evaluation framework designed to assess whether end-to-end autonomous driving models can generate behavior aligned with specific driving styles. Built upon the StyleDrive dataset, it leverages a closed-loop simulation environment based on NavSim to replay real-world scenarios under model control.
          </p>
          <p>
            Central to the benchmark is the <strong>Style-Modulated Predictive Driver Model Score (SM-PDMS)</strong>, a composite metric that evaluates not only safety and feasibility (e.g., time-to-collision, lane adherence) but also style alignment—such as comfort, assertiveness, and responsiveness—conditioned on target driving preferences (<em>Aggressive</em>, <em>Normal</em>, <em>Conservative</em>).
          </p>
          <p>
            To demonstrate applicability, the benchmark includes several style-conditioned baselines across different model families (e.g., MLP, Transformer, Diffusion-based). Experimental results show that incorporating driving style into model inputs significantly improves alignment with human demonstrations while maintaining safety and performance.
          </p>
          <p>
            The benchmark enables reproducible, quantitative comparison of personalized driving policies, providing a foundation for future research in controllable and human-aligned autonomous driving.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container" style="max-width: 860px;">
    <!-- Table 3 -->
    <div class="content has-text-centered">
      <h3 class="title is-4">DriveStyle Benchmark Main Results</h3>
      <p class="is-size-6">
        Style conditioning improves behavioral alignment, as reflected by higher SM-PDMS scores across most model families.
      </p>
    </div>
    <div class="table-container" style="overflow-x: auto;">
      <table class="table is-bordered is-striped is-fullwidth is-size-7">
        <thead>
          <tr>
            <th>Models</th>
            <th>NC ↑</th>
            <th>DAC ↑</th>
            <th>TTC ↑</th>
            <th>Comf. ↑</th>
            <th>EP ↑</th>
            <th>SM-PDMS ↑</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>AD-MLP</td><td>92.63</td><td>77.68</td><td>83.83</td><td>99.75</td><td>78.01</td><td>63.72</td></tr>
          <tr><td>TransFuser</td><td>96.74</td><td>88.43</td><td>91.08</td><td>99.65</td><td>84.39</td><td>78.12</td></tr>
          <tr><td>WoTE</td><td>97.29</td><td>92.39</td><td>92.53</td><td>99.13</td><td>76.31</td><td>79.56</td></tr>
          <tr><td>DiffusionDrive</td><td>96.66</td><td>91.45</td><td>90.63</td><td>99.73</td><td>80.39</td><td>79.33</td></tr>
          <tr><td>AD-MLP-Style</td><td>92.38</td><td>73.23</td><td>83.14</td><td style="color: red;">99.90</td><td>78.55</td><td>60.02</td></tr>
          <tr><td>TransFuser-Style</td><td>97.23</td><td>90.36</td><td>92.61</td><td>99.73</td><td style="color: red;">84.95</td><td>81.09</td></tr>
          <tr><td>WoTE-Style</td>
              <td style="color: blue;">97.58</td>
              <td style="color: blue;">93.44</td>
              <td style="color: blue;">93.70</td>
              <td>99.26</td>
              <td>77.38</td>
              <td style="color: blue;">81.38</td>
          </tr>
          <tr><td>DiffusionDrive-Style</td>
              <td style="color: red;">97.81</td>
              <td style="color: red;">93.45</td>
              <td style="color: red;">92.81</td>
              <td style="color: red;">99.85</td>
              <td style="color: red;">84.84</td>
              <td style="color: red;">84.10</td>
          </tr>
        </tbody>
      </table>
    </div>

        <!-- Table 4 -->
    <div class="content has-text-centered">
      <h3 class="title is-4">Closeness to Human Demonstrations</h3>
      <p class="is-size-6">
        Style-conditioned models exhibit lower L2 trajectory error compared to vanilla models, highlighting improved consistency with human driving behavior under preference-aware conditions.
      </p>
    </div>
    <div class="table-container" style="overflow-x: auto;">
      <table class="table is-bordered is-striped is-fullwidth is-size-7">
        <thead>
          <tr>
            <th>Models</th>
            <th>L2 (2s) ↓</th>
            <th>L2 (3s) ↓</th>
            <th>L2 (4s) ↓</th>
            <th>L2 (Avg) ↓</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>WoTE</td><td>0.733</td><td>1.434</td><td>2.349</td><td>1.506</td></tr>
          <tr><td>AD-MLP</td><td>0.503</td><td>1.262</td><td>2.383</td><td>1.382</td></tr>
          <tr><td>TransFuser</td><td>0.431</td><td>0.963</td><td>1.701</td><td>1.032</td></tr>
          <tr><td>DiffusionDrive</td><td>0.471</td><td>1.086</td><td>1.945</td><td>1.167</td></tr>
          <tr><td>WoTE-Style</td><td>0.673</td><td>1.340</td><td>2.223</td><td>1.412</td></tr>
          <tr><td>AD-MLP-Style</td><td>0.510</td><td>1.230</td><td>2.321</td><td>1.354</td></tr>
          <tr><td>TransFuser-Style</td><td>0.424</td><td>0.937</td><td>1.656</td><td>1.006</td></tr>
          <tr><td>DiffusionDrive-Style</td>
              <td style="font-weight: bold;">0.417</td>
              <td style="font-weight: bold;">0.940</td>
              <td style="font-weight: bold;">1.646</td>
              <td style="font-weight: bold;">1.001</td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- Spacer -->
    <br><br>

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Qualitative Case Study of Style Effects</h2>
        <figure class="image mt-4">
          <img src="static/images/Policy_Cases.png" alt="Alternative Annotation Framework" style="width: 100%; height: auto;">
        </figure>
        <p class="has-text-justified is-size-6 mt-3">
          Qualitative illustration of DiffusionDrive-Style predictions under different style conditions across identical scenarios. Left: Aggressive vs. Normal; Right: Conservative vs. Normal. Red lines indicate the model's predicted trajectory under the given style condition; green lines denote the ground-truth human trajectory. Clear behavioral differences emerge with style variation, reflecting the model's ability to adapt its outputs to driving preferences.
          <!-- <em>An alternative visualization of the annotation process, highlighting semantic extraction, behavioral labeling, and human-in-the-loop fusion.</em> -->
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Dataset View</h2>
        <figure class="image mt-4">
          <img src="static/images/alter.png" alt="Alternative Annotation Framework" style="width: 100%; height: auto;">
        </figure>
        <p class="has-text-justified is-size-6 mt-3">
          <!-- <em>An alternative visualization of the annotation process, highlighting semantic extraction, behavioral labeling, and human-in-the-loop fusion.</em> -->
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->

<!-- End image carousel -->












<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code> @article{hao2025styledrive,
  title={StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving},
  author={Hao, Ruiyang and Jing, Bowen and Yu, Haibao and Nie, Zaiqing},
  journal={arXiv preprint arXiv:2506.23982},
  year={2025}
}</code></pre>
    </div>
</section>


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
<script>
  bulmaCarousel.attach('#video-carousel', {
    slidesToScroll: 1,
    slidesToShow: 1,
    autoplay: false,  // 你可以改成 true
    loop: true,
    pauseOnHover: true,
    navigation: true
  });
</script>

<a id="back-to-top" title="Back to top">
  <i class="fas fa-arrow-up"></i>
</a>


  </body>
</html>
